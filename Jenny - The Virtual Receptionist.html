<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jenny - The Virtual Receptionist</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="jenny---the-virtual-receptionist">Jenny - The Virtual Receptionist</h1>
<p>My name is Zac Brannelly, and throughout the next few blog posts I will be discussing the research, planning, and development of the Virtual Receptionist project at the Applied Artificial Intelligence Institute at Deakin University. Where the end result looks as follows and is ran on a kiosk, provided by our research partner Neo Products, placed in front of the doors at our office. This post will run through my experience (as an IBL student) developing this project and how the final product turned out.</p>
<p><img src="https://lh3.googleusercontent.com/LIXGkym0ePNh80iuyd2NWnKbTu3-98U39ee5ggNyp07fGj8_YoWW0o107Cl1Prgl3jgpYm-9bRF-=s800" alt="enter image description here"><br>
<em>Figure 1: Final look of the Virtual Receptionist project</em></p>
<h2 id="jennys-origin-story">Jenny’s Origin Story</h2>
<p>The Applied Artificial Intelligence Institute (A2I2) office in Burwood was lent a few different types of kiosks from Neo Products. Some were quite small and others stood as tall as a human. Since they have just been in the process of re-branding and have facial recognition technology on hand, they came up with the idea of a virtual receptionist that would be able to recognize the staff, open the doors for them, and notify staff of a visitor’s arrival.</p>
<h2 id="my-origin-story">My Origin Story</h2>
<p>I am here completing my IBL (Industry-based learning) placement from Swinburne University of Technology, where I’m enrolled in my third year of a Bachelors of Computer Science. So this project had begun development before I actually started working here. The original reason I was offered a placement interview (then position) was because of my experience working with the Unity game engine (which is the tool being used to develop the project).</p>
<p>So when I arrived at the start of January 2019, the prototype being developed by the summer intern over the holidays was already capable of using the webcam connected to the kiosk to perform face detection and recognition, show an animated avatar and greet the user via Windows Voice.</p>
<p><img src="https://lh3.googleusercontent.com/7ROfyqtT2TEBo4RKKAE1oqsZspIQX2laXFLbDFLOH-tdHdc0AAU_qHC6ihrB6S7z5OZlq4uHPFsP=s800" alt="Original Prototype" title="Prototype"><br>
<em>Figure 2: Screen capture of the prototype when I arrived</em></p>
<p>As you can see above, it was very limited in capability, where it could only recognize people manually registered into a folder on disk, and it was able to use Windows text-to-speech to say "Hello and “Goodbye” when a user was recognized and lost respectively. So I was put to the task of making her being able to have a conversation with the user.</p>
<h2 id="building-jennys-ability-to-talk">Building Jenny’s ability to talk</h2>
<p>For Jenny to be able to converse with users, she needed to be able to understand human speech (speech-to-text) and convey her response with human speech (text-to-speech). The first attempt carried out by the summer intern for text-to-speech utilized Windows in-built text-to-speech engine, which was only capable of saying “Hello” and “Goodbye” to any users. The quality of this text-to-speech engine is not great and very robotic. For the quality standard we are trying to achieve with this project, a better text-to-speech engine was needed. As for speech-to-text there was no attempt made yet.</p>
<p>I was directed towards the Google Cloud Platform by my supervisor as a potential service that could be used for both of these features. This platform essentially had everything I needed to build this receptionist (including the implementation of her brain which is discussed next). So I started with Google’s speech-to-text service first. One of the hardest parts of implementing this service into the project was, although there was libraries written in the language that Unity uses, they were written for higher versions of this language that Unity doesn’t support (by default). So I instead decided to try use the REST API, which I had never done before, so it was a very good learning experience. I got hands on experience with creating and sending HTTP requests using C#'s asynchronous Web API and turning the response data into usable data (which in this case is just a String corresponding to the speech sent). This worked but was very slow.</p>
<p>Using the libraries instead would make it possible to stream audio straight to Google which would get responses more efficiently. So I put more time into trying to get the libraries to work with Unity. This paid off since I found out it was possible to increase the language version used by Unity. Now I could use the libraries directly in Unity, I was able to implement the speech-to-text service much more easily and it worked much more efficiently as predicted. More on how this is implemented will be discussed further in subsequent posts.</p>
<p><img src="https://lh3.googleusercontent.com/H-621uOpAoBlPt7X5ZFc9efRQjHQpLPF8ciUuDvb0LYKbYsJvbg0nUZBJ89JJneigs1C3lSfS0hM" alt="enter image description here"><br>
<em>Figure 3: Example of speech-to-text UI in final product</em></p>
<p>Next I moved onto implementing the text-to-speech service. Since this was part of the same platform as the speech-to-text service, it was much easier to implement as they follow the same architectural pattern. The biggest challenge of this feature was converting the audio data received from Google (WAV format) into audio data that could be played to the user by Unity (normalized floats). This was also a big learning curve for me since I also had no previous experience in this area. In the end I was able to successfully convert the WAV data into normalized floats and play the response to the user. More in-depth details of the implementation can be found in subsequent posts.</p>
<p><img src="https://lh3.googleusercontent.com/2VEhkFnMMfgVCLwzPB2vZWyn26xXt7QWn4Bz4dOgZ6QpFwdqmLi0zwjklDvDPGYzdhYWSpx5qm7N" alt="enter image description here"><br>
<em>Figure 4: Example of text-to-speech UI in final product</em></p>
<p>The final part of this stage was to implement a user interface that would visually tell the user when the receptionist was listening to them, what she recognized from their speech, and what she is saying in text form. In figure 3 &amp; 4 you can see an example of the UI created for speech-to-text and text-to-speech respectively. Where there is a cancel button for speech-to-text results (in-case they are way off), the text-to-speech captions are shown for 10 seconds and only two responses are shown at a time.</p>
<h2 id="building-jennys-brain">Building Jenny’s Brain</h2>
<p>Now that the receptionist was able to listen and talk, the next step was to create her “brain”! This “brain” is essentially a chatbot where the input is text (generated from the users voice) and the output is text (which will be sent to text-to-speech). The receptionist needed to be able to understand what the user was trying to say (no matter how they say their intention) and respond appropriately and accurately. So a simple set of if-statements was not going to cut it. I needed some natural language processing (NLP) to pull this off. This is when I was directed towards Dialogflow by my supervisor.</p>
<p>Dialogflow is a Google Cloud Platform product that is primarily used to create commercial chatbots such as customer service chatbots. It utilizes complex NLP techniques to detect the intent of a message sent by the user, and extract entities (useful data), which can then be used in the response and/or by a custom back-end. Getting your head around how this system works takes a little while and a fair amount of reading, but is simple once you have the basics down. So after some research I created the first version of Jenny’s “brain” using this system. Thankfully it followed the same architectural design as the other products I had used so implementation into the project was trivial. Majority of the work is done in a browser since the actual bot and how it responds is configured via the Dialogflow console website. This version of Jenny was able to greet users by name and respond to phrases such as “How are you going?” with “I’m doing well thanks, how are you?”. The main advantage to this product is its ability to match the user’s intention with predefined intentions based off a few training phrases. More on this implementation can be found in further blog posts.</p>
<p>Another suggestion made by my supervisor was to try out another chatbot engine called ChatScript which was being utilized in another project (of similar features to this project). This engine worked a lot differently to Dialogflow, hence being another big learning curve (this whole project was a learning curve to be fair haha). As it’s name suggests, it uses scripts to determine the response of the bot and regular expression <em>like</em> intention matching. Also this engine isn’t a cloud service, meaning we either needed to host a server that runs this engine or run the engine directly in the application. The biggest advantage of using this engine over Dialogflow is the script language syntax is not far from common programming languages and given I’m a programmer I felt more at home using this engine. Plus it allowed the use of variables and if-statements which made making more complex dialogue much more intuitive than in Dialogflow. In the end this is the engine that was used in the final product to control the dialogue for all the user interactions. Although, after retrospection, Dialogflow may have been a better fit. More details of the implementation can be found in further blog posts.</p>
<h2 id="increasing-jennys-intelligence-artificially">Increasing Jenny’s Intelligence (Artificially)</h2>
<p>In the final implementation of her “brain”, the ChatScript side is capable of handling the workflows that were designed during the planning stages. These workflows aren’t too complex and are easily handled by using scripts. What the scripts doesn’t handle well is general small talk. With a lot of work and large scripts it could handle these interactions, but this is just a waste of time for such a little feature. Dialogflow on the other hand has a feature where it will automatically generate responses to small talk if detected, essentially with a flip of a switch. So to use this in conjunction with ChatScript, I just sent the user’s request to Dialogflow whenever no intention was matched in ChatScript. This worked well when the user would ask questions such as “How are you feeling?” to the receptionist.</p>
<p>Another goal of this project was to make a bot that was interesting to talk to and to show off AI. This is where I was directed to a paper by Facebook AI researchers where they implemented a system called DrQA that could answer general questions using NLP, machine learning and an entire cache of Wikipedia. The first hurdle in integrating this system into the project was that it required Linux to run correctly. Given that this project’s platform was mainly Windows this was an issue. This is where I learned about Docker. By putting the DrQA system into a Docker container I was able to run it on Windows. Where the next hurdle was discovered which was RAM space. Since it loaded the entire Wikipedia cache into RAM, I needed to run this system on a cloud server with enough space. After finding a free (for a limited time) service that could run the system, I implemented the system as a web-server where questions could be sent using a HTTP end-point and the answer found would be in the response. This was also my first time implementing a system that used web end-points which was interesting and surprisingly easy. The last part was just to integrate this system into ChatScript which would trigger the next user input to be sent to DrQA if the user asks “Can I ask you a question?”. More on this implementation will be discussed further in subsequent posts.</p>
<p>Given that the above system only works on servers with large amounts of RAM, and my trial running out, I decided to find an alternative to DrQA that wouldn’t need so much resources. This is where I found DuckDuckGo’s Instant Answer API. This worked in a similar fashion to my system where you send the question to a web end-point and the answer would be returned in a JSON response. This JSON response would also sometimes come back with fields that could be used to load an image related to the question such as a logo. This service wasn’t as accurate as DrQA but it did the job. So in the final product this is the default service used for question answering, with the option to change when a DrQA server was available.</p>
<p><img src="https://lh3.googleusercontent.com/QhYZSKoqpoedH-zyiP3rgCPA91WkCGd28STOM4INki1QrJwtaw_W2ttKff6LxNkAJVbc3T9gsI-2=s500" alt="enter image description here"><br>
<em>Figure 5: Example of image displayed after question about Apples asked</em></p>
<h2 id="the-final-product">The Final Product</h2>
<p>The next few sections will show the final entire functionality of Jenny as a virtual receptionist. Each section is a workflow that was designed and planned before implementation. This planning and design process will be discussed much further in subsequent posts.</p>
<h3 id="registration-for-visitors">Registration (for visitors)</h3>
<p><img src="https://lh3.googleusercontent.com/ixuNrgex8TeGxQ1YuqMOZYonGFzCedTAaVmpCQrBgMLsCFUtIn1JrXi43stRt8FwftmiWcBgnUBG" alt="enter image description here"><br>
<em>Figure 6: GIF of registration workflow</em></p>
<p>The above GIF (Figure 6) is a complete screen capture of the final registration workflow. Where the receptionist will ask what your name is, and it will show a UI dialog where you can either enter your name via on-screen keyboard or you can say it (not shown in this GIF). When saying your name via voice, it utilizes Dialogflow’s powerful NLP techniques to extract your name from the phrase, so even if you say “My name is Zac” it will only extract “Zac”. Next it expands the size of the webcam feed and starts a countdown to take a snap of you. This snap is then sent for encoding to the Face Encoder Docker and then if the encodings are received then they are registered in a MongoDB database. The receptionist will then move on to the Visitor Arrival Notification workflow.</p>
<h3 id="visitor-arrival-notification">Visitor Arrival Notification</h3>
<p><img src="https://lh3.googleusercontent.com/TAdVqJwcOVCBLmRKbTcFVJ08hEXvobT9ut5SvqgQkZdZQz6PQKEWEE7AWs5QsV3-30oRvmKDebK7" alt="enter image description here"><br>
<em>Figure 7: GIF of look up workflow</em></p>
<p><img src="https://lh3.googleusercontent.com/NmliQbYMsJeqYBk16mM0FaytimJhZ2XDVK4H5BFSVcuYo26Jc5kuhJi9Mmlz4VyhfZET5EaovmUG" alt="enter image description here"><br>
<em>Figure 8: Example of message sent during look up workflow to staff member</em></p>
<p>The above GIF (Figure 7) shows a screen capture of the entire Look Up workflow. Where the receptionist will first ask you who you are here to see while showing a dialog with a complete list of the staff in the office. In this dialog, you can scroll and select, you can search via text and select, or you can use your voice to search (and auto-select if only one result). Then it will ask you for the purpose of your visit, where we narrowed it down to a few options for ease of use. Here you can too use voice to select an option, if the voiced option wasn’t recognized then “Other” is automatically selected. It will then send a Slack message to the staff member selected earlier. An example of the message sent can be seen in Figure 8. It will then go to an idle state where it waits for input from the user.</p>
<h3 id="contact-staff-via-slack">Contact Staff (via Slack)</h3>
<p><img src="https://lh3.googleusercontent.com/f3Hki3Fm6aOKUC5chfMoBBS2IO8UUIDVa0W_EWfHU1f_C941EhOAhzRM2INyfrbvRaVzefYhHWmT" alt="enter image description here"><br>
<em>Figure 9: GIF of contact staff workflow</em></p>
<p><img src="https://lh3.googleusercontent.com/cFxxcwsZLhp7JkfbCKW48hduyV-9oizsKnWOHNaIZjlPkXzQA7Cwf6uSM9hmZx9zSaDtpSb01QBF" alt="enter image description here"><br>
<em>Figure 10: Example of message sent to staff during contact staff workflow</em></p>
<p>The above GIF (Figure 9) is a complete screen capture of the Contact Staff workflow. Where the user must ask the receptionist if they can message someone. Then she will ask what would you like to say, where a dialog will be shown. You can either say or enter the message via the on-screen keyboard. Then she will ask who you would like to send the message to and will show a dialog similar to the one shown in the Look Up workflow (has all the same features). Then it will send the message via Slack to the staff member and will notify if it was successful or not. An example of a message can be seen in Figure 10 (this is not the same message sent in Figure 9). It will then go back to an idle state where it waits for instructions from the user.</p>
<h3 id="remove-user-information">Remove User Information</h3>
<p><img src="https://lh3.googleusercontent.com/WFSOsPuG1vNuLjAX159PKS1OknukVmLYIVwhhZnk8xu92aOW5nMof1f3LZiRRsh3pACyMuN3CukJ" alt="enter image description here"><br>
<em>Figure 11: GIF of remove user information workflow</em></p>
<p>The GIF above (Figure 11) shows the entire workflow for removing the user’s data. The user must first ask “Please delete my data” and then the receptionist will show a dialog with a “Delete” button and a “Keep” button. You can either press the buttons or respond with “Yes” or “No” via voice input. When the user chooses to delete their data, it will remove their face from the MongoDB database and go back to the idle state. It does not immediately stop recognizing them, instead when they comeback next time, they won’t be recognized.</p>
<h3 id="ask-a-question-entertainment">Ask a question (Entertainment)</h3>
<p><img src="https://lh3.googleusercontent.com/w0lULDASKMa9q8m4WjQJ29tV_j5iFcKoCnQpFAMUfazCna5AsviMSBv5_GfNIUdHog9CBiZ8Wcwr" alt="enter image description here"><br>
<em>Figure 12: GIF of user asking “Who is Margot Robbie” in ask question workflow</em></p>
<p>The GIF above (Figure 12) shows the complete workflow for asking the receptionist a general question. Where the user must first say “Can I ask you a question?” and then the receptionist will say “Sure go ahead”. Then the user must say their question (which in this case was “Who is Margot Robbie?”). The receptionist will then use DuckDuckGo (since we have no DrQA server available) to answer the question. As you can see, it also came along with a picture in the response so this was displayed to the user.</p>
<h3 id="music-recognition-entertainment">Music Recognition (Entertainment)</h3>
<p><img src="https://lh3.googleusercontent.com/xPnWrZuX8zNfFFFF0qkAVHm95uoZvCAtDJgKKcTNmhKVxlAiqdx0mYgnoxHAWpnOyDvta5oMu8Zk" alt="enter image description here"><br>
<em>Figure 13: GIF of the music recognition workflow where is successfully guessed the song</em></p>
<p>The GIF above (Figure 13) shows the complete workflow of the guess the song function. First you ask the receptionist “can you guess this song?” and she will respond with “I definitely can” and then will begin recording straight after. The user must have their song ready to play before asking her to do this. She will record for around 10 seconds and then will send the audio to ACRCloud for recognition. If she gets a response she will say the name and artist (which in this case was Johnny Ran Away by Tones and I) other wise she will ask if you would like her to try again.</p>
<h3 id="registering-staff-via-slack">Registering Staff (via Slack)</h3>
<p><img src="https://lh3.googleusercontent.com/7ODip2ze7T7l4lWCLeNK0wrqUYby8Z4W8IelSx4tKJK4Q8BB8kwWFTwd1-GfIHIBddUIGpxJrkbk=s900" alt="enter image description here"><br>
<em>Figure 14: Screen capture of registering a staff member via Slack</em></p>
<p>In the final product, to register staff into the system, this is done via the receptionist’s Slack bot. Implemented using Node.js and Dialogflow, the user must ask the bot to register a face and then upload either an image of the staff member (with their name as the filename, see Figure 14) or a zip file containing multiple images of staff members (so you can bulk register if needed).</p>
<h3 id="removing-users-via-slack">Removing Users (via Slack)</h3>
<p><img src="https://lh3.googleusercontent.com/zXm7dQLy1Sqswn094Q4GbmZcw8YzX55EU1dtdipQzfHvgpk6JUB58Hn6xgl8hOoiD9cJTY70zRTb" alt="enter image description here"><br>
<em>Figure 15: Screen capture of listing registered faces in Slack</em></p>
<p><img src="https://lh3.googleusercontent.com/zD2TxyWf8yKEY1Gd8rl48_rUj7EGN7UYk9pdapHsAoalfJPg7mBv7oj9HdHSfOUxjCWSgn-K3ZMY" alt="enter image description here"><br>
<em>Figure 16: Screen capture of removing a registered user via Slack</em></p>
<p>To remove faces from Jenny’s memory in the final product, Slack must be used to ask her to delete the face. Implemented using the same Node.js application as described above, the user must ask to remove a face and then specify either the name of the face or the ID of the face which can be obtained by asking the bot to list faces (as seen in figure 15).</p>
<h2 id="conclusion">Conclusion</h2>
<p>I really enjoyed working on this project. It had many ups and downs, but overall I learned a lot more from this project than originally thought. I got hands on experience with so many different technologies (some that I didn’t even know existed). This experience will stay with me as I move on to other projects, especially the experience with Google Cloud, Node.js, Tensorflow and Docker. Of course there are many things that, with hindsight, I should’ve done differently. For example I believe I should’ve continued using Dialogflow for the main “brain” since after more research and experimentation I could’ve reproduced the benefits of ChatScript using Dialogflow and made the receptionist much more human-like with its responses. It is also disappointing that we couldn’t give the receptionist the ability to open the door of the office but given the facial recognition could be fooled using a photograph of a staff member this was probably a smart move.</p>
<h2 id="whats-next-on-keeping-up-with-jenny...">Whats next on Keeping Up With Jenny…</h2>
<p>In the next blog post I will go into more depth about the planning process and what was actually planned before implementation began.</p>
<p>Thanks for reading :)</p>
</div>
</body>

</html>
